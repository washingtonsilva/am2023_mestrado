---
title: "R Notebook"
output: html_notebook
---

## How does lm() work?

`lm()` estimates regression coefficients using ordinary least squares, or OLS. The formula for this using matrix algebra is expressed as follows:

$$\hat{\beta} = (X'X)^{-1}X'Y $$

where X is the model matrix (our predictors as they appear in the model) and Y is the dependent variable. The prime symbol `'` means transpose the matrix. The `-1` means take the inverse. R was developed to make calculations such as these quite easy. 

Let's revisit model m2.

```{r}
formula(m2)
```

Here are the coefficients returned by `lm()`:

```{r}
round(coef(m2), 4)
```

Let's find these using the formula above. We can use `model.matrix()` to get X, `t()` to transpose, and `solve()` to take the inverse. Perform matrix multiplication with `%*%`

```{r}
X <- model.matrix(~ finsqft + bedroom + lotsize, data = homes)
Y <- log(homes$totalvalue)
B <- solve(t(X) %*% X) %*% t(X) %*% Y
round(B, 4)
```

While this is theoretically what `lm()` does, it actually uses more sophisticated methods for faster performance and protection against numeric instability. This page goes into more details if you're interested in learning more:

https://genomicsclass.github.io/book/pages/qr_and_regression.html


## ANOVA revisited

We can also use the `anova()` function to compare _nested models_. This means one model is a subset of another. For example, below we fit progressively more complicated models, building up to our model, `m2`: `log(totalvalue) ~ finsqft + bedroom + lotsize`

```{r}
mod_00 <- lm(log(totalvalue) ~ 1, data = homes) # intercept-only
mod_01 <- lm(log(totalvalue) ~ finsqft, data = homes)
mod_02 <- lm(log(totalvalue) ~ finsqft + bedroom, data = homes)
mod_03 <- lm(log(totalvalue) ~ finsqft + bedroom + lotsize, data = homes)
```

The `anova()` function allows us to compare these nested models. The null hypothesis is that two models are the same, in the sense they explain the same amount of variance in the response variable, `log(totalvalue)`. A low p-value provides evidence that the more complicated model with more variables is a better model. The usual way to use `anova()` to compare models is to list the smaller models first. Below are three tests: 

1. Model 2 vs Model 1
2. Model 3 vs Model 2
3. Model 4 vs Model 3

The end result is that model 4 is superior to the other three models.

```{r}
anova(mod_00, mod_01, mod_02, mod_03)
```

Notice this identical to calling `anova()` on the full model.

```{r}
anova(m2)
```

Another approach is using Type II sums of squares, where each variable is tested assuming _all other variables are in the model_. One approach to performing this test is the `drop1()` function. The name comes from the fact we're dropping one variable at a time. The null hypothesis is dropping the variable from the model has no effect. A low p-value provides evidence against this hypothesis. Below each are three tests:

1. Full model vs Full model without finsqft
2. Full model vs Full model without bedroom
3. Full model vs Full model without lotisze

Each test is soundly rejected. The full model is much better with all three predictors.

```{r}
drop1(m2, test = "F")
```

This test is also implemented in the `Anova()` function in the car package.

```{r}
# install.packages("car")
library(car)
Anova(m2)
```

## AIC and BIC

AIC (Akaike Information Criterion) is a statistic designed to help us choose a model with the best predictive power among a group of models. The value of  AIC doesn't really have any interpretation. It's meant to be compared to other AIC values. The lower the AIC the better. We can use the `AIC()` function in R to compare multiple models. For example, `mod_03` seems preferable to `mod_02` because the AIC value is so much smaller. 

```{r}
AIC(mod_02, mod_03)
```

AIC is the log-likelihood of the model multiplied by -2 with 2 x df added to it. The 2 x df part is a _penalty_. "df" is short for "degrees of freedom" and is the number of parameters estimated in the model. This includes the residual standard error. For example, `mod_03` has 5 degrees of freedom because there are 4 model coefficients and the residual standard error. We call this part a "penalty" because AIC can get bigger with more coefficients. 

Log-likelihood is the log-transformed likelihood. _Likelihood_ is the joint probability of the observed data as a function of the parameters of the chosen statistical model. Imagine turning the coefficients in the model like dials on a machine and trying to find the maximum likelihood. In other words, what combination of coefficients are most likely to produce the data we observed? R does not estimate linear model coefficients using maximum likelihood, but we can calculate the log likelihood after the fact using the `logLik()` function:

```{r}
logLik(mod_03)
```

We can then calculate AIC "by-hand" as follows.

```{r}
rbind("mod_02" = -2*logLik(mod_02) + 2*4, 
      "mod_03" = -2*logLik(mod_03) + 2*5)
```

The AIC is inclined to choose overly complex models, so some researchers prefer BIC (Bayesian Information Criterion), which places a bigger penalty on the number of predictors. Again use the `BIC()` function in the same way.

```{r}
BIC(mod_02, mod_03)
```

The BIC is calculated the same as the AIC but with a different penalty, `log(n)`, where n is the number of observations. Again we can calculate "by hand":

```{r}
rbind("mod_02" = -2*logLik(mod_02) + log(nrow(homes))*4, 
      "mod_03" = -2*logLik(mod_03) + log(nrow(homes))*5)
```

Of course, you don't have to choose one. You can use both AIC and BIC and report both. They will often choose the same models. 


## Collinearity

When predictors are highly correlated (ie, have strong linear relationships), the precision of coefficient estimates can decline. This phenomenon is often referred to as _collinearity_ or _multicollinearity_. Let's demonstrate with a toy example. First we generate two variables, x1 and x2, that are highly correlated:

```{r}
x1 <- seq(1,10,length = 100)
set.seed(123)
x2 <- 1 + 2*x1 + rnorm(100, sd = 0.2)
cor(x1, x2)  # calculate correlation; perfect correlation = 1
```

Now we generate a new variable, `y`, using `x1` and `x2` along with some noise and fit a linear model to recover the true coefficients of 2 and 3. Notice in the pairwise scatterplot that `x1` and `x2` both seem associated with `y` in the same way.

```{r}
set.seed(321)
y <- 0.5 + 2*x1 + 3*x2 + rnorm(100, sd = 10)
d_collinear <- data.frame(y, x1, x2)
pairs(d_collinear)
```

When we fit the model, our estimated coefficients are way off and have enormous standard errors. The "true" values are 2 and 3. The model estimates are about 12 and -2! Recall we interpret the `x1` coefficient as its effect on `y` holding `x2` constant. But since `x1` and `x2` are highly correlated, it's all but impossible to estimate the effect of `x1` while holding `x2` constant.

```{r}
mod_colinear <- lm(y ~ x1 + x2)
summary(mod_colinear)
```

The most common way to check for and quantify collinearity after fitting a model is calculating _variance inflation factors_ (VIF). The details of the calculation can be found with a web search, but the basic idea is that if one of the raw VIFs is greater than 10, then we may have evidence that collinearity is influencing coefficient estimates. Fortunately this is easy to do using the `vif()` function in the car package. The VIFs are sky high for our contrived predictors!

```{r}
library(car)
vif(mod_colinear)
```

The square root of the VIF can be interpreted as how much larger the standard error of the coefficient is relative to similar uncorrelated data. 

```{r}
vif(mod_colinear) |> sqrt()
```

This says the standard error for both variables is about 29 times larger than it would have been without collinearity. The best solution in this case would be to simply drop one of the variables. It appears `x1` is almost completely determined by `x2` and vice versa. Knowing one means you know the other. In the extreme case, when two variables are perfectly correlated, the model fitting procedure will return NA for one of the variables, as demonstrated below. Notice the message: "(1 not defined because of singularities)"

```{r}
x2 <- 2*x1  # perfect correlation
summary(lm(y ~ x1 + x2))
```

Let's check our `m2` model where we modeled `log(totalprice)` as a function of finsqft, bedroom and lotsize:

```{r}
vif(m2)
```

These VIFs look very good. We might suspect that finsqft and number of bedrooms could be highly correlated, but the VIF checks out.

One approach to addressing collinearity concerns is to use a data reduction technique such as principal components analysis (PCA). When it works, this method basically takes several variables and reduces them to one or two summary scores. This may be preferable to arbitrarily dropping variables. 

For models that are intended for making predictions, collinearity is not much of a concern. 


## ggplot2 code for creating plot of simulated data

Here's how to make the simulation plot using ggplot2. I find base R graphics easier for this type of plot. 

```{r}
sim2 <- simulate(m2, nsim = 50)
library(ggplot2)
library(tidyr)
sim2 %>% 
  pivot_longer(everything(), 
               names_to = "simulation", 
               values_to = "totalvalue") %>% 
  ggplot() +
  geom_density(mapping = aes(x = totalvalue, group = simulation),
               color = "grey80") + 
  geom_density(mapping = aes(x = log(totalvalue)),
               data = homes)  

```



## How to make a function for simulating and plotting data for a linear model

**base R**

We basically copy and paste the original code in between the curly braces of the `function()` function. We call the function `plot_sims` but you can name it whatever you like. The changes are to the model name and number of simulations. We generalize those with arguments: `mod` and `nsim`. When we fit a model with `lm`, the data is stored with the model object by default and can be accessed as mod$model. The first column contains the model response, so we can access it with `[,1]` and use it to draw the density of the observed data.


```{r}
plot_sims <- function(mod, nsim){
  sim <- simulate(mod, nsim = nsim)
  plot(density(mod$model[,1]))
  for(i in 1:nsim)lines(density(sim[[i]]), col = "grey80")
}
# try the function
plot_sims(mod = m2, nsim = 20)
```

**ggplot2**

Same idea as the previous function: we want to copy the original code in between the curly braces of the `function()` function. Except we now want to preface functions with their package name (eg, ggplot2::) so we can use the function without having the packages loaded. We also do away with the pipe `%>%` since it comes from yet another package (magrittr) but is accessible when tidyr is loaded. We extract the name of the response using `resp <- names(mod$model)[1]` and then use the use the `.data` pronoun (`.data[[resp]]`) from rlang package to use it with ggplot.


```{r}
plot_sims <- function(mod, nsim){
  sim1 <- simulate(mod, nsim = nsim)
  resp <- names(mod$model)[1]
  sim1 <- tidyr::pivot_longer(sim1, everything(), 
               names_to = "simulation", 
               values_to = "totalvalue") 
  ggplot2::ggplot() +
  ggplot2::geom_density(mapping = ggplot2::aes(x = totalvalue, 
                                               group = simulation), 
                        color = "grey80", 
                        data = sim1) + 
  ggplot2::geom_density(mapping = ggplot2::aes(x = .data[[resp]]),
                        data = mod$model)  
}

plot_sims(m2, nsim = 65)

```

**bayesplot package**

The bayesplot package has a function for this called `ppc_dens_overlay`, but it's a little weird to use for linear models because it was designed to be used with Bayesian models. However it's not that hard to deploy. The first argument is simply the observed data. The second argument expects the simulations per row as opposed to per column. So we need to transpose, which we can do with the `t()` function. The result is a clean plot with the y-axis unlabeled since it really isn't needed and a legend to distinguish between observed and simulated (or replicated) data. 

```{r}
# install.packages("bayesplot")
library(bayesplot)
ppc_dens_overlay(log(homes$totalvalue), t(sim2))
```


**performance package**

The easystats collection of packages "aims to provide a unifying and consistent framework to tame, discipline, and harness the scary R statistics and their pesky models." https://easystats.github.io/easystats/ One of the packages is called {performance}, which provides the `check_predictions()` function for simulating data from a model and comparing it to the distribution of the original data. Two potential drawbacks at the time of this writing:

1. Can be slow for large data sets
2. log transformed data needs to be added as a variable to the data and modeled directly, as opposing to doing it on-the-fly as `log(totalvalue)`

Quick demo. Note the warning: "Minimum value of original data is not included in the replicated data." This says the model is not generating data as small as the smallest value in the original data, which is 9600.

```{r}
# install.packages("performance")
# install.packages("see")
library(performance)

# add log(totalvalue) to data and fit new model
homes$logtotalvalue <- log(homes$totalvalue)
m2a <- lm(logtotalvalue ~ finsqft + bedroom + lotsize, data = homes)
check_predictions(m2a)
```


